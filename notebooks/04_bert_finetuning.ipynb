{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3540619-2944-4792-bfe6-19d53d997ba9",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b00b28-7a7c-47d6-8848-0fa03799a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import torch\n",
    "from transformers import BertweetTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments \n",
    "#Roberta model, Auto Tokenizer\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6f96f-b42d-470f-bd91-a73225fe29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hydrogen_small.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58facd5-e600-4443-8a2b-ae51ad295713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361fb2b-09a6-473e-82a7-796a39dced0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe3791-cbdc-4f4c-9f93-b473c8138028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb316177-dc63-4c33-86e9-2b745bb50bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe0ed5-6c89-4b47-854f-d4701a87499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d9601-4db6-4c75-91f8-c3635f4ce92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e1ec6-1562-4665-8ff7-1554d0620197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a795f74-5efe-4b49-851d-e9122f4b17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\n",
    "    'Irrelevant': 0, # Negative = 0\n",
    "    'Relevant': 1 # Positive = 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a864a-768e-4906-a9eb-b1b48c42fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720bbec-88b7-4499-8b2d-c8193f581b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"label\"] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ea03f-9c3a-4e39-87d3-842130d504b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688154a-12b3-4a3f-a9e1-4e0747d9a163",
   "metadata": {},
   "source": [
    "## 4.2 Two pre-trained BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413d8a0-6080-4b83-af5d-d77cb987b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"].values\n",
    "y = df[\"label\"].values\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "stratify=y, test_size=0.3, random_state=random_state)\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Testing set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00e284-f26d-46bd-8b39-3769a01c23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(\"Train dataset:\", train_ds)\n",
    "print(\"Test dataset:\", test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152e39a-810e-48dd-8a7b-f80c839acba3",
   "metadata": {},
   "source": [
    "## 4.2.1 Bertweet Model (Vinai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c06247-b74a-45f5-b284-cb8fbd9d832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = \"vinai/bertweet-base\"\n",
    "tokenizer1 = BertweetTokenizer.from_pretrained(model_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c362c7-b070-49fd-9bc0-e97c73ebfa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that is applied to all samples in the dataset.\n",
    "\n",
    "def tokenize_bertweet(batch):\n",
    " # We set truncation=True to truncate (cut off) messages that are too long.\n",
    " # NOTE: Not all models require this, you may get a warning indicating that it has no effect.\n",
    " # Padding is set to True if the model requires a fixed sequence length.\n",
    "    return tokenizer1(batch['text'], truncation=True, padding=True)\n",
    "# Apply to both the training and testing datasets.\n",
    "# We set batched to True which can enable parallel processing, however on my machine I found\n",
    "# it did not scale to a greater number of threads.\n",
    "train_ds_bertweet = train_ds.map(tokenize_bertweet, batched=True)\n",
    "test_ds_bertweet = test_ds.map(tokenize_bertweet, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18014f5d-259f-483b-a142-4cdb89c43ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028c077-b94a-49d6-944a-abaec6014d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the resources for any existing model has been freed.\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "# Download/load the base model. We use the \"vinai/bertweet-base\" model here.\n",
    "# Set the number of labels to the number of unique labels in the dataframe, which is 2.\n",
    "# Set the problem type to single label classification, since we want one class for each sample.\n",
    "model1 = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name1,\n",
    "    num_labels=df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2a8b0-51c4-4cd5-846f-5439a4ac19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(\n",
    "    labels, preds, average=\"binary\", pos_label=1)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0e884-91af-4bfb-948b-3281b5966284",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopping_model1 = RobertaForSequenceClassification.from_pretrained(\n",
    " model_name1,\n",
    " num_labels=df[\"label\"].nunique(),\n",
    " problem_type=\"single_label_classification\")\n",
    "EarlyStopping_model1.train()\n",
    "EarlyStopping_training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=10,\n",
    " per_device_train_batch_size=16,\n",
    " per_device_eval_batch_size=64,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=1e-5,\n",
    " weight_decay=0.01,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " # Added for early stopping.\n",
    " metric_for_best_model = \"loss\",\n",
    " load_best_model_at_end = True\n",
    ")\n",
    "EarlyStopping_trainer1 = Trainer(\n",
    " model=EarlyStopping_model1,\n",
    " args=EarlyStopping_training_args,\n",
    " train_dataset=train_ds_bertweet,\n",
    " eval_dataset=test_ds_bertweet,\n",
    " processing_class=tokenizer1,\n",
    " data_collator=DataCollatorWithPadding(tokenizer1),\n",
    " compute_metrics=compute_metrics,\n",
    " callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "EarlyStopping_trainer1.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e3c44-d061-4f9e-9d8a-71bc729b2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model1.eval()\n",
    "# Evaluate the datasets.\n",
    "train_results_bertweet = EarlyStopping_trainer1.evaluate(train_ds_bertweet)\n",
    "test_results_bertweet = EarlyStopping_trainer1.evaluate(test_ds_bertweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652a365-1f5b-44fd-aef6-c3e9920d712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname_bertweet, results_bertweet):\n",
    " print(f\"{setname_bertweet} Set Accuracy:\", round(results_bertweet[\"eval_accuracy\"], 3))\n",
    " print(f\"{setname_bertweet} Set Precision:\", round(results_bertweet[\"eval_precision\"], 3))\n",
    " print(f\"{setname_bertweet} Set Recall:\", round(results_bertweet[\"eval_recall\"], 3))\n",
    " print(f\"{setname_bertweet} Set F1 score:\", round(results_bertweet[\"eval_f1\"], 3))\n",
    "display_evaluation(\"Training\", train_results_bertweet)\n",
    "display_evaluation(\"Testing\", test_results_bertweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44960afc-e7ca-438a-aac6-addcb9f7a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, y, stratify=y, test_size=0.3, random_state=random_state)\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds_bertweet = Dataset.from_pandas(train_df)\n",
    "test_ds_bertweet = Dataset.from_pandas(test_df)\n",
    "train_ds_bertweet = train_ds_bertweet.map(tokenize_bertweet, batched=True)\n",
    "test_ds_bertweet = test_ds_bertweet.map(tokenize_bertweet, batched=True)\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Testing set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9f559-165b-4dc3-ac2a-66ed5f3a621c",
   "metadata": {},
   "source": [
    "## 4.2.2 Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e531d-1144-4f5b-8383-c2c3e88f5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name2  = 'roberta-base'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f40dc-48dc-4d74-8e07-8697c46411c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_roberta(batch):\n",
    "    return tokenizer2(batch['text'], truncation=True, padding=True)\n",
    "\n",
    "train_ds_roberta = train_ds.map(tokenize_roberta, batched=True)\n",
    "test_ds_roberta = test_ds.map(tokenize_roberta, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17a908-aca7-4f4c-922f-6c57b16b6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd5f75-31da-4342-8427-02c123eafd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a890985-cb38-40ad-b35a-2a04b818e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name2,\n",
    "    num_labels=df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990c6e2-a733-4d44-aa69-9fbfee6fc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopping_model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    " model_name2,\n",
    " num_labels=df[\"label\"].nunique(),\n",
    " problem_type=\"single_label_classification\")\n",
    "EarlyStopping_model2.train()\n",
    "EarlyStopping_training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=10,\n",
    " per_device_train_batch_size=16,\n",
    " per_device_eval_batch_size=64,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=2e-5,\n",
    " weight_decay=0.01,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " # Added for early stopping.\n",
    " metric_for_best_model = \"loss\",\n",
    " load_best_model_at_end = True\n",
    ")\n",
    "EarlyStopping_trainer2 = Trainer(\n",
    " model=EarlyStopping_model2,\n",
    " args=EarlyStopping_training_args,\n",
    " train_dataset=train_ds_roberta,\n",
    " eval_dataset=test_ds_roberta,\n",
    " processing_class=tokenizer2\n",
    "    ,\n",
    " data_collator=DataCollatorWithPadding(tokenizer2),\n",
    " compute_metrics=compute_metrics,\n",
    " callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "EarlyStopping_trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d9fe5-73c3-4f9b-af7d-0d727527a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.eval()\n",
    "# Evaluate the datasets.\n",
    "train_results_roberta = EarlyStopping_trainer2.evaluate(train_ds_roberta)\n",
    "test_results_roberta = EarlyStopping_trainer2.evaluate(test_ds_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd44bb4-556d-4e2d-b915-8949761cab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname_roberta, results_roberta):\n",
    " print(f\"{setname_roberta} Set Accuracy:\", round(results_roberta[\"eval_accuracy\"], 3))\n",
    " print(f\"{setname_roberta} Set Precision:\", round(results_roberta[\"eval_precision\"], 3))\n",
    " print(f\"{setname_roberta} Set Recall:\", round(results_roberta[\"eval_recall\"], 3))\n",
    " print(f\"{setname_roberta} Set F1 score:\", round(results_roberta[\"eval_f1\"], 3))\n",
    "display_evaluation(\"Training\", train_results_roberta)\n",
    "display_evaluation(\"Testing\", test_results_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf1967-a6e4-4eb5-8c39-758811c1858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, y, stratify=y, test_size=0.3, random_state=random_state)\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds_roberta = Dataset.from_pandas(train_df)\n",
    "test_ds_roberta = Dataset.from_pandas(test_df)\n",
    "train_ds_roberta = train_ds_roberta.map(tokenize_roberta, batched=True)\n",
    "test_ds_roberta = test_ds_roberta.map(tokenize_roberta, batched=True)\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Testing set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dd2fa-5513-4219-aa50-500a2c536d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns (matrix, tokens)\n",
    "def compute_attention_matrix(tokenizer, model, text):\n",
    "    # Feed into the model, you could also grab the token embedding directly\n",
    "    # from the dataset, in which case this step would be unnecessary. We want\n",
    "    # the output in Tensor format that we can feed to the model, so we use\n",
    "    # return_tensors=\"pt\" (PyTorch Tensor). Lastly, send the tensor to\n",
    "    # whichever device the model is located on. This is unnecessary if you\n",
    "    # are running purely on the CPU, but needed for models on GPUs.\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    # We use torch.no_grad() to ensure the weights in the model are unchanged.\n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens, output_attentions=True)\n",
    "    # Stack layers. Depending on your model, this may have no effect.\n",
    "    # Move it back to the GPU if it was previously on the GPU.\n",
    "    attentions = torch.stack(pred.attentions).cpu()\n",
    "    # Remove the batch dimension, as there is only a zero value there.\n",
    "    attentions = attentions.squeeze(1)\n",
    "    # Average over the transformer layers and heads.\n",
    "    attentions = attentions.mean(dim=0).mean(dim=0)\n",
    "    # attentions now contains a matrix of importance from every token to every\n",
    "    # other token. e.g. if the message contained 10 tokens, it would be 10x10.\n",
    "    # Select the predicted class.\n",
    "    pred_class = pred.logits.cpu().argmax(-1).item()\n",
    "    # Also return a string representation of the tokens in the message.\n",
    "    # Plotting the integer token IDs would not be very meaningful. \n",
    "    token_strs = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]) \n",
    "    return (attentions, pred_class, token_strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65774f37-5984-45d9-8095-db6af2c44bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attentions, tokens, title):\n",
    " # Enlarge figure to take up more of the width.\n",
    " plt.figure(figsize=(10, 8))\n",
    " plt.title(title)\n",
    " # Plot heatmap.\n",
    " sns.heatmap(\n",
    " attentions, # Plot our attention matrix.\n",
    " xticklabels=tokens, # Display token names on X axis.\n",
    " yticklabels=tokens, # Display token names on Y axis.\n",
    " cmap='binary', # Black for low, white for high\n",
    " cbar=True # Display colour bar.\n",
    " )\n",
    " \n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7bafe-42b3-4917-9172-e25541289a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention_matrix(tokenizer, model, text, model_name=\"Model\"):\n",
    " attention, pred_class, tokens = compute_attention_matrix(tokenizer, model, text)\n",
    " pred_label = \"Positive\" if pred_class == 1 else \"Negative\"\n",
    " title=f\"{model_name}\\n{text}\\nPredicted class: {pred_label}\"\n",
    " plot_attention(attention, tokens, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9ed6d-82e6-4a6b-bd1c-c493424cd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizer1, model1, df[df[\"label\"] == 0].iloc[18][\"text\"], model_name=\"BERTweet Model\")\n",
    "display_attention_matrix(tokenizer2, model2, df[df[\"label\"] == 0].iloc[18][\"text\"], model_name=\"RoBERTa Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4759094-a252-49a6-9768-97c1080e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizer1, model1, df[df[\"label\"] == 0].iloc[72][\"text\"], model_name=\"BERTweet Model\")\n",
    "display_attention_matrix(tokenizer2, model2, df[df[\"label\"] == 0].iloc[72][\"text\"], model_name=\"RoBERTa Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3835c-f8cb-44bb-a64b-1d627ceee512",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bertweet = EarlyStopping_trainer1.predict(test_ds_bertweet)\n",
    "pred_roberta = EarlyStopping_trainer2.predict(test_ds_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2af22-7f95-46a2-90e7-2c1764bc843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor, apply softmax, and convert back to a numpy array.\n",
    "pred_probs_bertweet = torch.nn.functional.softmax(torch.Tensor(pred_bertweet.predictions)).numpy()\n",
    "pred_probs_roberta = torch.nn.functional.softmax(torch.Tensor(pred_roberta.predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4dc02f-42bf-4740-b74f-58eb661431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ROC index. Recall y_test contains our original labels for the testing set.\n",
    "roc_index_bertweet = roc_auc_score(y_test, pred_probs_bertweet[:, 1])\n",
    "roc_index_roberta = roc_auc_score(y_test, pred_probs_roberta[:, 1])\n",
    "# Compute the ROC curve.\n",
    "fpr_bertweet,tpr_bertweet, thresholds_bertweet = roc_curve(y_test, pred_probs_bertweet[:,1])\n",
    "fpr_roberta,tpr_roberta, thresholds_roberta = roc_curve(y_test, pred_probs_roberta[:,1])\n",
    "# And plot it on a line graph, similarly to what we did in previous weeks.\n",
    "plt.plot(fpr_bertweet, tpr_bertweet, label=\"BERTweet Model: {:.3f}\".format(roc_index_bertweet),\n",
    "color='red', lw=0.5)\n",
    "plt.plot(fpr_roberta, tpr_roberta, label=\"RoBerta Model: {:.3f}\".format(roc_index_roberta),\n",
    "color='navy', lw=0.5)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic for positive sentiment\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b56ea-869f-4444-b2a4-caecd50aadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf_df = pd.read_csv('tfidf_features_small.csv')\n",
    "tfidf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e55528-0e1b-4a84-a945-3467c4cbb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df\n",
    "\n",
    "df= pd.read_csv(\"hydrogen_small.csv\")\n",
    "y= df['label'].values\n",
    "\n",
    "random_state = 42\n",
    "test_set_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=test_set_size, stratify=y, \n",
    "                                                    random_state=random_state)\n",
    "model = LogisticRegression(random_state=random_state)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5db81-9cb6-4182-b1f9-11676ee1cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# classification report on test data\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3acc39-987d-4180-9804-a086c7daf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'Irrelevant': 0, 'Relevant': 1}\n",
    "y_test_bin = np.array([label_map[y] for y in y_test])\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC/AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_pred_proba) \n",
    "roc_auc = roc_auc_score(y_test_bin, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd8eb0-3b39-4b7a-9b7a-b2ccbcd0550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# BERTweet ROC\n",
    "plt.plot(fpr_bertweet, tpr_bertweet,\n",
    "         label=f'BERTweet Model (AUC = {roc_index_bertweet:.3f})',\n",
    "         color='red', lw=1.5)\n",
    "#Roberta ROV\n",
    "plt.plot(fpr_roberta, tpr_roberta,\n",
    "         label=f'Roberta Model (AUC = {roc_index_roberta:.3f})',\n",
    "         color='blue', lw=1.5)\n",
    "\n",
    "plt.plot(fpr, tpr,\n",
    "         label=f'Logistic Regression (AUC = {roc_auc:.3f})',\n",
    "         color='black', lw=1.5)\n",
    "\n",
    "plt.plot([0,1], [0,1], color='grey', linestyle='--', lw=1)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.title(\"ROC Curve Comparison: BERTweet vs Logistic Regression\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
