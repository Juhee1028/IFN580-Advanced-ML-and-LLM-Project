{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79ea35-2e83-4f78-8a6c-28ce304ba2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"tfidf_features.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a93c95-7fdc-4b03-93c7-10f6b374bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c36b0-3897-481b-b013-5c515bb19c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed14957-9b60-40bc-82b0-bad8e8a2281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fe131-a9ee-4af3-96b0-05221ed9a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=10\n",
    "X_np = X.to_numpy()\n",
    "\n",
    "dims = [2, 5, 10, 20, 50, 100, 200]\n",
    "mean_dists = []\n",
    "\n",
    "for d in dims:\n",
    "    Xd = X_np[:, :d] # Select the first d features\n",
    "    dists = []\n",
    "    n = len(Xd) # Number of samples\n",
    "\n",
    "    #Compute pairwise Euclidean distances\n",
    "    for i in range(n): # Select the first sample\n",
    "        for j in range(i + 1, n): # Select the second sample (bigger than i to avoid duplicates and self-comparisons)\n",
    "            dist = np.linalg.norm(np.array(Xd[i]) - np.array(Xd[j])) #Euclidean Distance\n",
    "            dists.append(dist)\n",
    "    mean_dists.append(np.mean(dists))\n",
    "\n",
    "mean_dists_df = pd.DataFrame({\"Dimension\": dims, \"Mean Euclidean Distance\": mean_dists})\n",
    "print(mean_dists_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c3db6-c902-4e0d-81ac-c565cfcef762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(mean_dists_df[\"Dimension\"], mean_dists_df[\"Mean Euclidean Distance\"], marker='o')\n",
    "plt.xlabel(\"Number of dimensions\")\n",
    "plt.ylabel(\"Mean Euclidean Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038fd1da-1128-4ef0-a1f2-6037b7081aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=10\n",
    "X_np = X.to_numpy()\n",
    "\n",
    "dims = [2, 5, 10, 20, 50, 100, 200]\n",
    "log_dists = []\n",
    "\n",
    "for d in dims:\n",
    "    Xd = X_np[:, :d]\n",
    "    n = len(Xd)\n",
    "\n",
    "    # Initialise nearsest and farthest distances\n",
    "    d_min = [float('inf')] * n # Smallest distance -> start from inf\n",
    "    d_max = [0] * n # Largest distance -> start from 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = np.linalg.norm(Xd[i] - Xd[j])\n",
    "\n",
    "            # Update nearest distances \n",
    "            if dist < d_min[i]: d_min[i] = dist\n",
    "            if dist < d_min[j]: d_min[j] = dist\n",
    "\n",
    "            # Update farthest distances\n",
    "            if dist > d_max[i]: d_max[i] = dist\n",
    "            if dist > d_max[j]: d_max[j] = dist\n",
    "\n",
    "    d_min = np.array(d_min)\n",
    "    d_max = np.array(d_max)\n",
    "    \n",
    "    valid = (d_min > 0) & (d_min < np.inf) # d_min must be > 0 except isolated, self distances and infinites\n",
    "    d_min_valid = d_min[valid]\n",
    "    d_max_valid = d_max[valid]\n",
    "\n",
    "    log_dists.append(np.mean(np.log((d_max_valid - d_min_valid) / (d_min_valid))))\n",
    "\n",
    "log_df = pd.DataFrame({\"Dimension\": dims, \"log((max-min)/min)\": log_dists})\n",
    "print(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc415b7-4c46-4f59-a6c7-1826357bac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(log_df[\"Dimension\"], log_df[\"log((max-min)/min)\"], marker='o')\n",
    "plt.xlabel(\"Number of dimensions\")\n",
    "plt.ylabel(\"log((max-min)/min)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881d2c6-cbe7-41de-bf14-f1418ecabba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comps_num = np.arange(1, 200+1)\n",
    "comps_variance = []\n",
    "\n",
    "for num_components in comps_num:\n",
    "    pca = PCA(n_components=num_components, random_state=10)\n",
    "    pca.fit(X.values)\n",
    "    total_explained_variance = sum(pca.explained_variance_ratio_)\n",
    "    comps_variance.append(total_explained_variance)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.plot(comps_num, comps_variance, marker='o')\n",
    "plt.xlabel(\"# Components\")\n",
    "plt.ylabel(\" Cumulative explained variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ebbf83-e44b-44aa-9fda-77ff5e8bd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(comps_variance, index=comps_num, columns=[\"total_explained_variance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00e99c-a8fb-46df-a59a-af2a261d44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_each = pca.explained_variance_ratio_[:5]\n",
    "top5_total = sum(top5_each)\n",
    "\n",
    "print(\"Top 5 each:\", top5_each)\n",
    "print(\"Top 5 explained variance:\", top5_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2a164-0493-4f25-b5f8-04a4f649190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_tsne = PCA(n_components=50, random_state=10)\n",
    "X_pca50 = pca_tsne.fit_transform(X.values)\n",
    "print(\"Shape after PCA:\", X_pca50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79359bd9-5218-45d3-8171-05d4b715b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "random_state = 10\n",
    "\n",
    "tsne_comps_num = np.arange(1, 3+1)\n",
    "tsne_comps_divergence = []\n",
    "\n",
    "for num_components in tsne_comps_num:\n",
    "    tsne = TSNE(n_components=num_components, random_state=10)\n",
    "    tsne.fit(X_pca50)\n",
    "    tsne_comps_divergence.append(tsne.kl_divergence_)\n",
    "    print(f\"KL Divergence for {num_components}: {tsne.kl_divergence_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a05f0b-d91e-469e-bde6-736ae17123af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_perplexity_num = np.arange(5, 50+1, 5)\n",
    "tsne_perplexity_divergence = []\n",
    "\n",
    "for perplexity in tsne_perplexity_num:\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=10)\n",
    "    tsne.fit(X_pca50)\n",
    "    tsne_perplexity_divergence.append(tsne.kl_divergence_)\n",
    "    print(f\"KL Divergence for perplexity of {perplexity}: {tsne.kl_divergence_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a348e9-c713-49c7-9249-8e3f6d12ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(12, 6))\n",
    "plt.grid()\n",
    "plt.plot(tsne_perplexity_num, tsne_perplexity_divergence, marker='o')\n",
    "plt.xlabel(\"Perplexity\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a45651-5036-4500-81fd-a64a94f56d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 2 Dimensionality\n",
    "pca = PCA(n_components=2, random_state=10)\n",
    "X_pca = pca.fit_transform(X.values)\n",
    "print(\"Total explained variance:\", sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfd256-d95b-4527-adc5-fa4db735f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(12, 6))\n",
    "plt.grid()\n",
    "\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], s=20)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e60740-c3de-4fd1-b8e3-fe635fc136fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 3 Dimensionality\n",
    "pca = PCA(n_components=3, random_state=10)\n",
    "X_pca = pca.fit_transform(X.values)\n",
    "print(\"Total explained variance:\", sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb0d5e-cecb-498d-9be1-bdb39da6f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], s=20)\n",
    "ax.set(xlabel=\"PC1\", ylabel=\"PC2\", zlabel=\"PC3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c92abc-f7da-4791-a1c9-cf415ce010c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE 2 Dimensionality\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=random_state)\n",
    "X_tsne = tsne.fit_transform(X.values)\n",
    "print(\"KL Divergence with 2 components and perplexity of 50:\", tsne.kl_divergence_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022dbdcb-8c8b-47be-887d-07ed11b8e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(12, 6))\n",
    "plt.grid()\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=20)\n",
    "plt.xlabel(\"C1\")\n",
    "plt.ylabel(\"C2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8c19d-8dc4-4375-bed0-6bf9f78a5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_perplexity_num = np.arange(5, 50+1, 5)\n",
    "tsne_perplexity_divergence = []\n",
    "\n",
    "for perplexity in tsne_perplexity_num:\n",
    "    tsne= TSNE(n_components=3, perplexity=perplexity, random_state=random_state)\n",
    "    tsne.fit(X.values)\n",
    "    tsne_perplexity_divergence.append(tsne.kl_divergence_)\n",
    "    print(f\"KL Divergence for perplexity of {perplexity}: {tsne.kl_divergence_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c450f-6cea-4dd4-af53-cf3c2028f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE 3 Dimensionality\n",
    "tsne = TSNE(n_components=3, perplexity=50, random_state=random_state)\n",
    "X_tsne = tsne.fit_transform(X.values)\n",
    "print(\"KL Divergence with 3 components and perplexity of 50:\", tsne.kl_divergence_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39897fc-d8eb-42ab-8797-7bbaeda5a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], s=20)\n",
    "ax.set(xlabel=(\"C1\"), ylabel=\"C2\", zlabel=\"C3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62366161-e912-4242-ba5e-7a63e1df19b6",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d5449-645a-4abd-9b10-3296a105e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as ns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"kick.csv\", na_filter=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391332a-0fbd-421b-9de1-85c8577f8162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VehOdo\n",
    "print(df['VehOdo'].describe())\n",
    "print(df['VehOdo'].value_counts())\n",
    "print(\"=====================\")\n",
    "print(df['VehOdo'].unique())\n",
    "\n",
    "#MMRAcquisitionAuctionAveragePrice\n",
    "print(df['MMRAcquisitionAuctionAveragePrice'].describe())\n",
    "print(df['MMRAcquisitionAuctionAveragePrice'].value_counts())\n",
    "print(\"=====================\")\n",
    "print(df['MMRAcquisitionAuctionAveragePrice'].unique())\n",
    "\n",
    "#Make\n",
    "print(df['Make'].describe())\n",
    "print(df['Make'].value_counts())\n",
    "print(\"=====================\")\n",
    "print(df['Make'].unique())\n",
    "\n",
    "#WarrantyCost\n",
    "print(df['WarrantyCost'].describe())\n",
    "print(df['WarrantyCost'].value_counts())\n",
    "print(\"=====================\")\n",
    "print(df['WarrantyCost'].unique())\n",
    "\n",
    "#IsBadBuy\n",
    "print(df['IsBadBuy'].describe())\n",
    "print(df['IsBadBuy'].value_counts())\n",
    "print(\"=====================\")\n",
    "print(df['IsBadBuy'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d7e3e-7ce9-47c1-910c-96824c5035d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of VehOdo\n",
    "regdens_dist = sns.histplot(df['VehOdo'].dropna(), kde=True, stat=\"density\",\n",
    "kde_kws=dict(cut=3))\n",
    "plt.show()\n",
    "# Distribution of MMRAcquisitionAuctionAveragePrice\n",
    "medhhinc_dist = sns.histplot(df['MMRAcquisitionAuctionAveragePrice'].dropna(), kde=True, stat=\"density\",\n",
    "kde_kws=dict(cut=3))\n",
    "plt.show()\n",
    "# Distribution of WarrantyCost\n",
    "meanhhsz_dist = sns.histplot(df['WarrantyCost'].dropna(), kde=True, stat=\"density\",\n",
    "kde_kws=dict(cut=3))\n",
    "plt.show()\n",
    "#countplot of IsBadBuy(binary)\n",
    "sns.countplot(x='IsBadBuy', data=df)\n",
    "plt.title('Distribution of IsBadBuy (Target Variable)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b7c1-f2c9-4797-beb6-456bcc54671b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VehOdo_threshold = [1000,2500,5000]\n",
    "for t in VehOdo_threshold:\n",
    "    df[f'HasError_VehOdo_{t}'] = df['VehOdo'] < t\n",
    "    g = sns.FacetGrid(df, col=f'HasError_VehOdo_{t}')\n",
    "    g = g.map(plt.hist, 'MMRAcquisitionAuctionAveragePrice', bins=100)\n",
    "    plt.suptitle(f\"VehOdo < {t}\",y=1.05)\n",
    "    plt.show()\n",
    "    \n",
    "VehOdo_threshold = [110000,130000,150000]\n",
    "for t in VehOdo_threshold:\n",
    "    df[f'HasError_VehOdo_{t}'] = df['VehOdo'] > t\n",
    "    g = sns.FacetGrid(df, col=f'HasError_VehOdo_{t}')\n",
    "    g = g.map(plt.hist, 'MMRAcquisitionAuctionAveragePrice', bins=100)\n",
    "    plt.suptitle(f\"VehOdo > {t}\",y=1.05)\n",
    "    plt.show()   \n",
    "\n",
    "VehOdo_threshold = [1000,2500,5000]\n",
    "for t in VehOdo_threshold:\n",
    "    df[f'HasError_VehOdo_{t}'] = df['VehOdo'] < t\n",
    "    g = sns.FacetGrid(df, col=f'HasError_VehOdo_{t}')\n",
    "    g = g.map(plt.hist, 'WarrantyCost', bins=100)\n",
    "    plt.suptitle(f\"VehOdo < {t}\",y=1.05)\n",
    "    plt.show()\n",
    "    \n",
    "VehOdo_threshold = [110000,130000,150000]\n",
    "for t in VehOdo_threshold:\n",
    "    df[f'HasError_VehOdo_{t}'] = df['VehOdo'] > t\n",
    "    g = sns.FacetGrid(df, col=f'HasError_VehOdo_{t}')\n",
    "    g = g.map(plt.hist, 'WarrantyCost', bins=100)\n",
    "    plt.suptitle(f\"VehOdo > {t}\",y=1.05)\n",
    "    plt.show()   \n",
    "\n",
    "df['HasError_MMRA']=(df['MMRAcquisitionAuctionAveragePrice'] < 500)|(df['MMRAcquisitionAuctionAveragePrice'] > 20000)\n",
    "g = sns.FacetGrid(df, col='HasError_MMRA')\n",
    "g = g.map(plt.hist, 'WarrantyCost', bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfba255-04fb-4a1f-b0de-058453f9a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before\n",
    "print(\"Row VehOdo before dropping errorneous rows\", len(df))\n",
    "df = df[(df['VehOdo'] >= 5000)&(df['VehOdo'] <= 110000)]\n",
    "# after\n",
    "print(\"Row VehOdo after dropping errorneous rows\", len(df))\n",
    "\n",
    "#MMRAcquisitionAuctionAveragePrice\n",
    "print(\"Row MMRA before dropping errorneous rows\", len(df))\n",
    "df = df[(df['MMRAcquisitionAuctionAveragePrice'] >= 500)&(df['MMRAcquisitionAuctionAveragePrice'] <= 20000)]\n",
    "print(\"Row MMRA after dropping errorneous rows\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead05f20-cc65-45bd-af12-56260974076b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of VehOdo\n",
    "regdens_dist = sns.histplot(df['VehOdo'].dropna(), kde=True, stat=\"density\",\n",
    "kde_kws=dict(cut=3))\n",
    "plt.show()\n",
    "# Distribution of MMRAcquisitionAuctionAveragePrice\n",
    "medhhinc_dist = sns.histplot(df['MMRAcquisitionAuctionAveragePrice'].dropna(), kde=True, stat=\"density\",\n",
    "kde_kws=dict(cut=3))\n",
    "plt.show()\n",
    "# Distribution of WarrantyCost\n",
    "meanhhsz_dist = sns.histplot(df['WarrantyCost'].dropna(), kde=True, stat=\"density\",\n",
    "kde_kws=dict(cut=3))\n",
    "plt.show()\n",
    "#countplot of IsBadBuy(binary)\n",
    "sns.countplot(x='IsBadBuy', data=df)\n",
    "plt.title('Distribution of IsBadBuy (Target Variable)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2587d5-b418-418b-b360-a7a9ae2c0cbb",
   "metadata": {},
   "source": [
    "## 2.2 Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aebf15-ca8e-4910-a52d-8fbc72307fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# take 3 variables and drop the rest. copy the dataframe to avoid warnings later\n",
    "df2 = df[['VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'WarrantyCost']].copy() # convert df2 to matrix\n",
    "X = df2.to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddfa5a-a8b0-410c-9021-22e0a9bf9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "for seed in [1, 5, 10, 42, 100]:\n",
    "    model = KMeans(n_clusters=3, random_state=seed)\n",
    "    model.fit(X)\n",
    "    print(f\"Seed {seed} -> Inertia: {model.inertia_}\")\n",
    "    print(\"Centroid locations:\")\n",
    "    for centroid in model.cluster_centers_:\n",
    "        print(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24052816-9030-4ba3-8f96-8c0d340e346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a several n_clusters\n",
    "for k in range(1,11):\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(X)\n",
    "    print(\"The number of cluster:\", k)\n",
    "    print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "    print(\"Centroid locations:\")\n",
    "    for centroid in model.cluster_centers_:\n",
    "         print(centroid)\n",
    "    print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7183c2d-a950-436a-a4e5-25addc8d531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "inertia_vals = []\n",
    "for k in range(1,11):\n",
    " # train clustering with the specified K\n",
    " model = KMeans(n_clusters=k, random_state=42)\n",
    " model.fit(X)\n",
    " \n",
    " # append model to cluster list\n",
    " clusters.append(model)\n",
    " inertia_vals.append(model.inertia_)\n",
    " \n",
    "# plot the inertia vs K values\n",
    "plt.plot(range(1,11), inertia_vals, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc28700-7887-4f40-8a87-22fa3889f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(clusters[1])\n",
    "print(\"Silhouette score for k=2\", silhouette_score(X, clusters[1].predict(X)))\n",
    "print(\"=========================================\")\n",
    "print(clusters[2])\n",
    "print(\"Silhouette score for k=3\", silhouette_score(X, clusters[2].predict(X)))\n",
    "print(\"=========================================\")\n",
    "print(clusters[3])\n",
    "print(\"Silhouette score for k=4\", silhouette_score(X, clusters[3].predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f65a1-4ead-4c65-a3a8-0c26b3ab5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of K=3 clustering solution\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "model.fit(X)\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    " print(centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5759d-cecc-4d89-b279-01135adacda8",
   "metadata": {},
   "source": [
    "## 2.3 The optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22314436-9f6c-4c55-b7b6-a0359a36dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(X)\n",
    "df2['Cluster_ID'] = y\n",
    "# how many in each\n",
    "print(\"Cluster membership\")\n",
    "print(df2['Cluster_ID'].value_counts()) \n",
    "\n",
    "# pairplot\n",
    "# added alpha value to assist with overlapping points\n",
    "cluster_g = sns.pairplot(df2, hue='Cluster_ID', diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b202a2-6f90-491c-91e3-72b04c8a9511",
   "metadata": {},
   "source": [
    "## 2.4 New Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9543b1-b282-463d-9425-3028712bc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d9aad-c0b0-4a3e-bd8d-7ba315719580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df[['VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'WarrantyCost', 'Make']].copy()\n",
    "print(\"Task 2.4. data:\")\n",
    "print(df_4.info())\n",
    "print(df_4['Make'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ccec0-a4aa-4d6d-9f6e-26ac35f13596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# convert string labels to numerical\n",
    "le = LabelEncoder()\n",
    "df_4['Make_encoded'] = le.fit_transform(df_4['Make'].values)\n",
    "print(df_4[['Make', 'Make_encoded']].head(10))\n",
    "df_4 = df_4[['VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'WarrantyCost', 'Make_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf4ee0-71e0-49e3-8013-1553e1c08e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to matrix\n",
    "X = df_4.to_numpy()\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de51049-5208-4f7d-8bf8-d468ffb8595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "cost_vals = []\n",
    "# this process is computationally expensive and may take some time\n",
    "for k in range(1,11):\n",
    "    # train clustering with the specified K\n",
    "    model = KPrototypes(n_clusters=k, random_state=42, n_jobs=-1)\n",
    "    model.fit_predict(X, categorical=[3])\n",
    " \n",
    "    # append model to cluster list\n",
    "    clusters.append(model)\n",
    "    cost_vals.append(model.cost_)\n",
    "# plot the cost vs K values\n",
    "plt.plot(range(1,11), cost_vals, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7717d6-8d7c-4351-844f-5d9b11359a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = [[row[0], row[1], row[2]] for row in X] # Variables of X with numeric datatype\n",
    "X_cat = [[row[3]] for row in X] # variables of X with categorical datatype\n",
    "#K=3\n",
    "model = clusters[2] # cluster[1] holds the K-prtotypes model with K=3\n",
    "# Calculate the Silhouette Score for the numeric and categorical variables seperately\n",
    "silScoreNums = silhouette_score(X_num, model.fit_predict(X, categorical=[2]),\n",
    "metric='euclidean')\n",
    "print(\"Silscore for numeric variables:\", silScoreNums)\n",
    "silScoreCats = silhouette_score(X_cat, model.fit_predict(X, categorical=[2]),\n",
    "metric='hamming')\n",
    "print(\"Silscore for categorical variables:\", silScoreCats)\n",
    "# Average the silhouette scores\n",
    "silScore = (silScoreNums + silScoreCats) / 2\n",
    "print(\"The avg silhouette score for k=2:\", silScore)\n",
    "\n",
    "\n",
    "#K=4\n",
    "model = clusters[3] # cluster[1] holds the K-prtotypes model with K=4\n",
    "# Calculate the Silhouette Score for the numeric and categorical variables seperately\n",
    "silScoreNums = silhouette_score(X_num, model.fit_predict(X, categorical=[3]),\n",
    "metric='euclidean')\n",
    "print(\"Silscore for numeric variables:\", silScoreNums)\n",
    "silScoreCats = silhouette_score(X_cat, model.fit_predict(X, categorical=[3]),\n",
    "metric='hamming')\n",
    "print(\"Silscore for categorical variables:\", silScoreCats)\n",
    "# Average the silhouette scores\n",
    "silScore = (silScoreNums + silScoreCats) / 2\n",
    "print(\"The avg silhouette score for k=4:\", silScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6c600-54ea-4eb8-8b53-5b7a72e74e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optical K=4\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "model = clusters[3]\n",
    "y=model.fit_predict(X, categorical=[3])\n",
    "df_4['Cluster_ID'] = y\n",
    "# how many records are in each cluster\n",
    "print(\"Cluster membership\")\n",
    "print(df_4['Cluster_ID'].value_counts())\n",
    "# pairplot the cluster distribution.\n",
    "cluster_g = sns.pairplot(df_4, hue='Cluster_ID', diag_kind='hist',\n",
    "                        height=3,       \n",
    "                        aspect=1,       \n",
    "                        plot_kws={'alpha': 0.6, 's': 30})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11a2aa-d120-4afe-b63f-e57fd54c7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('tsdm.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d99fba-16c1-4775-90d6-99814511ac99",
   "metadata": {},
   "source": [
    "## 3.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b785d41-7b2f-4550-a50f-e84d7a2b91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date type of OBSERVATION_DATE (datetime)\n",
    "df['OBSERVATION_DATE'] = pd.to_datetime(df['OBSERVATION_DATE'])\n",
    "\n",
    "# sort: essential time series prediction\n",
    "df = df.sort_values(['PADDOCK_ID','OBSERVATION_DATE']).reset_index(drop=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3b174-8d9a-4d81-a6c4-b9dd0b0f2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"PADDOCK_ID\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bab4f1-9da5-4546-a322-afb5116c4886",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075e3e9-71b3-4a3f-a27b-10e5e58bb7c2",
   "metadata": {},
   "source": [
    "- create_sequence(sequence, lookback, forecast_horizon, target_col)\n",
    "- data_prep(df, feature_columns, lookback, test_steps, target_col)\n",
    "- MyLSTMNet(nn.Module)\n",
    "- train_predict_model(model, n_epochs, lr, X_all, y_all, lengths, validation_split=0.2)\n",
    "- pred_eval(model, X, y, lengths, train_d, test_d, lookback, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea5a77-9b1f-426d-bba8-e46c4a9be61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_sequences function\n",
    "def create_sequences(sequence, lookback, forecast_horizon, target_col):\n",
    "    T, num_features = sequence.shape\n",
    "    X, y, lengths = [], [], []\n",
    "    pad_vector = np.zeros((lookback, num_features))\n",
    "\n",
    "    # Fixed-length lookback with pre-padding\n",
    "    for t in range(1, T - forecast_horizon + 1):\n",
    "        context = sequence[:t]\n",
    "        if len(context) > lookback:\n",
    "            context = context[-lookback:]\n",
    "\n",
    "        padded_context = pad_vector.copy()\n",
    "        padded_context[-len(context):] = context\n",
    "\n",
    "        X.append(padded_context)\n",
    "        y.append(sequence[t:t + forecast_horizon, target_col])\n",
    "        lengths.append(min(len(context), lookback))\n",
    "\n",
    "    return np.array(X), np.array(y), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ab60d-b262-4ee9-ad06-561818ccc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prep function: each location, split the data, scaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def data_prep(df, feature_columns, lookback, test_steps, target_col):\n",
    "    # prepare to store all training data\n",
    "    X_all, y_all = [], []\n",
    "    location_ids = [] # to track which location each sample comes from\n",
    "    test_data = [] # to store test data for each location\n",
    "    train_data = []\n",
    "    lengths_all = []\n",
    "\n",
    "    # Fit a global scaler\n",
    "    all_train_values = []\n",
    "    for _, group in df.groupby(\"PADDOCK_ID\"):\n",
    "        feature_values = group[feature_columns].values\n",
    "\n",
    "        if len(feature_values) > lookback + test_steps:\n",
    "            all_train_values.append(feature_values[:-test_steps])\n",
    "        all_train_values = np.vstack(all_train_values)\n",
    "\n",
    "        global_scaler = MinMaxScaler()\n",
    "        global_scaler.fit(all_train_values)\n",
    "\n",
    "        for location_id, group in df.groupby(\"PADDOCK_ID\"):\n",
    "            feature_values = group[feature_columns].values\n",
    "\n",
    "            if len(feature_values) <= 194:\n",
    "                continue\n",
    "\n",
    "            # split and scale\n",
    "            train_sample = global_scaler.transform(feature_values[:-test_steps])\n",
    "            test_sample = global_scaler.transform(feature_values[-test_steps:])\n",
    "\n",
    "            train_data.append((location_id, train_sample))\n",
    "            test_data.append((location_id, test_sample, global_scaler))\n",
    "\n",
    "            # prepare LSTM sequence data for training\n",
    "            X_location, y_location, lengths = create_sequences(train_sample, lookback, test_steps, target_col)\n",
    "\n",
    "            # append to the overall dataset\n",
    "            X_all.append(X_location)\n",
    "            y_all.append(y_location)\n",
    "            lengths_all.append(lengths)\n",
    "\n",
    "            # store location ID for tracking\n",
    "            location_ids.extend([location_id] * len(y_location))\n",
    "\n",
    "        # concatenate all locations' training data for model training\n",
    "        X_all = np.concatenate(X_all, axis=0)\n",
    "        y_all = np.concatenate(y_all, axis=0)\n",
    "        lengths_all = np.concatenate(lengths_all, axis=0)\n",
    "\n",
    "        X_all = X_all.reshape((X_all.shape[0], X_all.shape[1], X_all.shape[2]))\n",
    "\n",
    "        return(torch.Tensor(X_all), torch.Tensor(y_all),\n",
    "               torch.Tensor(lengths_all), train_data, test_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e28aef-44de-44c6-a520-8419fe5a101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LSTM network\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class MyLSTMNet(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer_size, num_layers, output_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_layer_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        packed_data = pack_padded_sequence(data, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Run through LSTM\n",
    "        packed_output, (hn, cn) = self.lstm(packed_data)\n",
    "\n",
    "        # Use the last layer's hidden state\n",
    "        last_hidden = hn[-1]\n",
    "\n",
    "        # apply dropout and final linear layer\n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219ddce-037d-4ac7-bd0b-7e1ba1ea9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Training Process\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "def train_predict_model(model, n_epochs, lr, X_all, y_all, lengths, validation_split=0.2):\n",
    "    batch_size = 32\n",
    "\n",
    "    # split data into train and validation sets\n",
    "    dataset = TensorDataset(X_all, y_all, lengths)\n",
    "    val_size = int(len(dataset) * validation_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch, lengths_batch in train_loader:\n",
    "            y_pred = model(X_batch, lengths_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation check every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_preds = model(X_all[train_set.indices], lengths[train_set.indices])\n",
    "                train_loss = loss_fn(train_preds, y_all[train_set.indices]).item()\n",
    "\n",
    "                val_preds = model(X_all[val_set.indices], lengths[val_set.indices])\n",
    "                val_loss = loss_fn(val_preds, y_all[val_set.indices]).item()\n",
    "\n",
    "                print(f\"Epoch {epoch+1}: train loss {train_loss:.4f}, val_loss {val_loss:.4f}\")\n",
    "\n",
    "                train_loss_history.append(train_loss)\n",
    "                val_loss_history.append(val_loss)\n",
    "\n",
    "                # save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = model.state_dict()\n",
    "\n",
    "    # restore best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_loss_history, val_loss_history, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad486e0-4e41-4d71-b2a0-03462b264246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of train loss\n",
    "def vis_train_loss(train_loss_history, val_loss_history):\n",
    "    epochs = range(0, n_epochs, 100)\n",
    "    plt.plot(epochs, train_loss_history, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Convergence')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e54f19-5b30-452d-a10b-21dde49417df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the RMSE: root_mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def pred_eval(model, X, y, lengths, train_d, test_d, lookback, target_col):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_preds = model(X, lengths)\n",
    "        print(\"Training RMSE:\", root_mean_squared_error(y.flatten().tolist(), train_preds.flatten().tolist()))\n",
    "        print(\"Training R2:\", r2_score(y.flatten().tolist(), train_preds.flatten().tolist()))\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        lengths_test = []\n",
    "\n",
    "        for count, (location_id, test_values, scaler) in enumerate(test_d):\n",
    "            train_values = train_d[count][1]\n",
    "            X_test.append(train_values[-lookback:])\n",
    "            y_test.append(test_values[:, target_col])\n",
    "\n",
    "            # append the actual lengths (just like the training phase)\n",
    "            lengths_test.append(len(train_values[-lookback:]))\n",
    "\n",
    "        X_test = torch.Tensor(np.array(X_test))\n",
    "        y_test = torch.Tensor(np.array(y_test))\n",
    "        lengths_test = torch.Tensor(lengths_test).long()\n",
    "        test_preds = model(X_test, lengths_test)\n",
    "\n",
    "        print(\"Test RMSE:\", root_mean_squared_error(y_test.flatten().tolist(), test_preds.flatten().tolist()))\n",
    "        print(\"Test R2:\", r2_score(y_test.flatten().tolist(), test_preds.flatten().tolist()))\n",
    "        \n",
    "        plt.figure(figsize = (10, 6))\n",
    "        plt.plot(y_test.flatten().tolist(), label=\"Expected Value\")\n",
    "        plt.plot(test_preds.flatten().tolist(), label=\"Predicted Value\")\n",
    "        plt.grid()\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add30e5-5369-46b1-8739-7241d5e40a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the last 5 timesteps of each paddock for test: test_steps=5\n",
    "\n",
    "lookback = 5\n",
    "test_steps = 5\n",
    "target_col = 0\n",
    "X_5, y_5, lengths_5, train_d_5, test_d_5 = data_prep(df, ['TSDM'], lookback, test_steps, target_col)\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_5.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479c489-9876-4459-be7b-86519549dd0f",
   "metadata": {},
   "source": [
    "## 3.2 Univariate LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f576fbc-2e8c-43b3-9501-c59a7de8e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate LSTM model (lookback=5, predict=5)\n",
    "num_features = X_5.shape[2]\n",
    "hidden_layer_size = 10\n",
    "output_size = test_steps\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2\n",
    "model_lstm_5 = MyLSTMNet(num_features, hidden_layer_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "print(model_lstm_5)\n",
    "print(\"============================================================\")\n",
    "\n",
    "# training the Univariate LSTM Model\n",
    "n_epochs = 201\n",
    "lr = 0.001\n",
    "train_loss_history_5, val_loss_history_5, model_lstm_5 = train_predict_model(model_lstm_5, n_epochs, lr, X_5, y_5, lengths_5)\n",
    "\n",
    "# visualisation of train loss\n",
    "vis_train_loss(train_loss_history_5, val_loss_history_5)\n",
    "print( )\n",
    "\n",
    "# RMSE of Univariate LSTM Model(lookback=5, predict=5)\n",
    "lookback = 5\n",
    "target_col = 0\n",
    "pred_eval(model_lstm_5, X_5, y_5, lengths_5, train_d_5, test_d_5, lookback, target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9144d-acc8-45a4-852d-4620ea0e0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find optimal hyperparameters\n",
    "\n",
    "lookback = 5\n",
    "test_steps = 5  # ensure the last 5 timesteps of each paddock for test: test_steps=5\n",
    "target_col = 0\n",
    "X_5, y_5, lengths_5, train_d_5, test_d_5 = data_prep(df, ['TSDM'], lookback, test_steps, target_col)\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_5.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_5.shape)\n",
    "print(\"============================================================\")\n",
    "\n",
    "# Univariate LSTM model (lookback=5, predict=5)\n",
    "num_features = X_5.shape[2]\n",
    "hidden_layer_size = 15\n",
    "output_size = test_steps\n",
    "num_layers = 1 # to check more simply and reduce overfiting risk\n",
    "dropout_prob = 0.2\n",
    "model_lstm_5 = MyLSTMNet(num_features, hidden_layer_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "print(model_lstm_5)\n",
    "print(\"============================================================\")\n",
    "\n",
    "# training the Univariate LSTM Model\n",
    "n_epochs = 201\n",
    "lr = 0.001\n",
    "train_loss_history_5, val_loss_history_5, model_lstm_5 = train_predict_model(model_lstm_5, n_epochs, lr, X_5, y_5, lengths_5)\n",
    "\n",
    "# visualisation of train loss\n",
    "vis_train_loss(train_loss_history_5, val_loss_history_5)\n",
    "print( )\n",
    "\n",
    "# RMSE of Univariate LSTM Model(lookback=5, predict=5)\n",
    "lookback = 5\n",
    "target_col = 0\n",
    "pred_eval(model_lstm_5, X_5, y_5, lengths_5, train_d_5, test_d_5, lookback, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01191446-76a8-4003-9739-6a92ea5cc0a3",
   "metadata": {},
   "source": [
    "## 3.3 Univariate LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc05530-2479-4d9d-9d11-b538cc800682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate LSTM Model (Lookback=10, Predict=5)\n",
    "lookback = 10\n",
    "test_steps = 5\n",
    "target_col = 0\n",
    "X_10, y_10, lengths_10, train_d_10, test_d_10 = data_prep(df, ['TSDM'], lookback, test_steps, target_col)\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_10.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_10.shape)\n",
    "print(\"============================================================\")\n",
    "\n",
    "num_features = X_10.shape[2]\n",
    "hidden_layer_size = 15\n",
    "output_size = test_steps\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2 # bigger number can make underfitting.\n",
    "model_lstm_10 = MyLSTMNet(num_features, hidden_layer_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "print(model_lstm_10)\n",
    "print(\"============================================================\")\n",
    "\n",
    "n_epochs = 201\n",
    "lr = 0.001\n",
    "train_loss_history_10, val_loss_history_10, model_lstm_10 = train_predict_model(model_lstm_10, n_epochs, lr, X_10, y_10, lengths_10)\n",
    "print( )\n",
    "vis_train_loss(train_loss_history_10, val_loss_history_10)\n",
    "\n",
    "print( )\n",
    "lookback = 10\n",
    "target_col = 0\n",
    "pred_eval(model_lstm_10, X_10, y_10, lengths_10, train_d_10, test_d_10, lookback, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd04c17-7ce8-485a-99d6-08bb9bdc7357",
   "metadata": {},
   "source": [
    "## 3.4 Multivariate LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36691259-4177-485c-a9d6-a88eeeda8e91",
   "metadata": {},
   "source": [
    "## 3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f29c13-be6c-4041-9d4b-5e8d6d86ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for no restriction to a fixed lookback\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_sequences(sequence, lookback, forecast_horizon, target_col, pad_value=0.0):\n",
    "    T, num_features = sequence.shape\n",
    "    X, y, lengths = [], [], []\n",
    "\n",
    "    if lookback > 0:\n",
    "        pad_vector = np.zeros((lookback, num_features))\n",
    "\n",
    "        for t in range(1, T - forecast_horizon + 1):\n",
    "            context = sequence[:t]\n",
    "            if len(context) > lookback:\n",
    "                context = context[-lookback:]\n",
    "            elif len(context) == 0:\n",
    "                continue  # to resolve null context problem\n",
    "\n",
    "            padded_context = pad_vector.copy()\n",
    "            padded_context[-len(context):] = context\n",
    "\n",
    "            X.append(padded_context)\n",
    "            y.append(sequence[t:t + forecast_horizon, target_col])\n",
    "            lengths.append(min(len(context), lookback))\n",
    "\n",
    "        return np.array(X), np.array(y), lengths\n",
    "    else:\n",
    "        for t in range(1, T - forecast_horizon + 1):\n",
    "            context = torch.tensor(sequence[:t], dtype=torch.float32)\n",
    "            \n",
    "            lengths.append(t)\n",
    "\n",
    "            X.append(context) # No manual padding\n",
    "            y.append(torch.tensor(sequence[t:t + forecast_horizon, target_col], dtype=torch.float32))\n",
    "\n",
    "        X_padded = pad_sequence(X, batch_first=True, padding_value=pad_value)\n",
    "        y_tensor = torch.stack(y)\n",
    "\n",
    "        return X_padded.numpy(), y_tensor.numpy(), lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f26dad-bd35-4dda-909e-aa0498bc4f54",
   "metadata": {},
   "source": [
    "## 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3485d-1d83-4bd2-970f-a4eba3925070",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 0 # mean no restriction of lookback\n",
    "test_steps = 5\n",
    "target_col = 0\n",
    "\n",
    "climate_features = ['TSDM','15D_AVG_DAILY_RAIN', '15D_AVG_MAX_TEMP', '15D_AVG_MIN_TEMP',\n",
    "                    '15D_AVG_RH_TMAX', '15D_AVG_RH_TMIN','15D_AVG_EVAP_SYN', '15D_AVG_RADIATION']\n",
    "\n",
    "X_f, y_f, lengths_f, train_d_f, test_d_f = data_prep(df, climate_features, lookback, test_steps, target_col)\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_f.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_f.shape)\n",
    "print(\"============================================================\")\n",
    "\n",
    "num_features = X_f.shape[2]\n",
    "hidden_layer_size = 20 # because of multivariate - need to increase hidden layer size\n",
    "output_size = test_steps\n",
    "n_epochs = 201 # to reduce running time\n",
    "lr = 0.001\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2\n",
    "\n",
    "model_lstm_f = MyLSTMNet(num_features, hidden_layer_size, num_layers, output_size, dropout_prob)\n",
    "print(model_lstm_f)\n",
    "print(\"============================================================\")\n",
    "\n",
    "train_loss_history_f, val_loss_history_f, model_lstm_f = train_predict_model(model_lstm_f, n_epochs, lr, X_f, y_f, lengths_f)\n",
    "print( )\n",
    "vis_train_loss(train_loss_history_f, val_loss_history_f)\n",
    "print( )\n",
    "pred_eval(model_lstm_f, X_f, y_f, lengths_f, train_d_f, test_d_f, lookback, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3540619-2944-4792-bfe6-19d53d997ba9",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b00b28-7a7c-47d6-8848-0fa03799a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import torch\n",
    "from transformers import BertweetTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments \n",
    "#Roberta model, Auto Tokenizer\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6f96f-b42d-470f-bd91-a73225fe29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hydrogen_small.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58facd5-e600-4443-8a2b-ae51ad295713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361fb2b-09a6-473e-82a7-796a39dced0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe3791-cbdc-4f4c-9f93-b473c8138028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb316177-dc63-4c33-86e9-2b745bb50bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe0ed5-6c89-4b47-854f-d4701a87499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d9601-4db6-4c75-91f8-c3635f4ce92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e1ec6-1562-4665-8ff7-1554d0620197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a795f74-5efe-4b49-851d-e9122f4b17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\n",
    "    'Irrelevant': 0, # Negative = 0\n",
    "    'Relevant': 1 # Positive = 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a864a-768e-4906-a9eb-b1b48c42fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720bbec-88b7-4499-8b2d-c8193f581b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"label\"] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ea03f-9c3a-4e39-87d3-842130d504b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688154a-12b3-4a3f-a9e1-4e0747d9a163",
   "metadata": {},
   "source": [
    "## 4.2 Two pre-trained BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413d8a0-6080-4b83-af5d-d77cb987b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"].values\n",
    "y = df[\"label\"].values\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "stratify=y, test_size=0.3, random_state=random_state)\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Testing set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00e284-f26d-46bd-8b39-3769a01c23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(\"Train dataset:\", train_ds)\n",
    "print(\"Test dataset:\", test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152e39a-810e-48dd-8a7b-f80c839acba3",
   "metadata": {},
   "source": [
    "## 4.2.1 Bertweet Model (Vinai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c06247-b74a-45f5-b284-cb8fbd9d832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = \"vinai/bertweet-base\"\n",
    "tokenizer1 = BertweetTokenizer.from_pretrained(model_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c362c7-b070-49fd-9bc0-e97c73ebfa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that is applied to all samples in the dataset.\n",
    "\n",
    "def tokenize_bertweet(batch):\n",
    " # We set truncation=True to truncate (cut off) messages that are too long.\n",
    " # NOTE: Not all models require this, you may get a warning indicating that it has no effect.\n",
    " # Padding is set to True if the model requires a fixed sequence length.\n",
    "    return tokenizer1(batch['text'], truncation=True, padding=True)\n",
    "# Apply to both the training and testing datasets.\n",
    "# We set batched to True which can enable parallel processing, however on my machine I found\n",
    "# it did not scale to a greater number of threads.\n",
    "train_ds_bertweet = train_ds.map(tokenize_bertweet, batched=True)\n",
    "test_ds_bertweet = test_ds.map(tokenize_bertweet, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18014f5d-259f-483b-a142-4cdb89c43ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028c077-b94a-49d6-944a-abaec6014d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the resources for any existing model has been freed.\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "# Download/load the base model. We use the \"vinai/bertweet-base\" model here.\n",
    "# Set the number of labels to the number of unique labels in the dataframe, which is 2.\n",
    "# Set the problem type to single label classification, since we want one class for each sample.\n",
    "model1 = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name1,\n",
    "    num_labels=df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2a8b0-51c4-4cd5-846f-5439a4ac19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(\n",
    "    labels, preds, average=\"binary\", pos_label=1)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0e884-91af-4bfb-948b-3281b5966284",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopping_model1 = RobertaForSequenceClassification.from_pretrained(\n",
    " model_name1,\n",
    " num_labels=df[\"label\"].nunique(),\n",
    " problem_type=\"single_label_classification\")\n",
    "EarlyStopping_model1.train()\n",
    "EarlyStopping_training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=10,\n",
    " per_device_train_batch_size=16,\n",
    " per_device_eval_batch_size=64,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=1e-5,\n",
    " weight_decay=0.01,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " # Added for early stopping.\n",
    " metric_for_best_model = \"loss\",\n",
    " load_best_model_at_end = True\n",
    ")\n",
    "EarlyStopping_trainer1 = Trainer(\n",
    " model=EarlyStopping_model1,\n",
    " args=EarlyStopping_training_args,\n",
    " train_dataset=train_ds_bertweet,\n",
    " eval_dataset=test_ds_bertweet,\n",
    " processing_class=tokenizer1,\n",
    " data_collator=DataCollatorWithPadding(tokenizer1),\n",
    " compute_metrics=compute_metrics,\n",
    " callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "EarlyStopping_trainer1.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e3c44-d061-4f9e-9d8a-71bc729b2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model1.eval()\n",
    "# Evaluate the datasets.\n",
    "train_results_bertweet = EarlyStopping_trainer1.evaluate(train_ds_bertweet)\n",
    "test_results_bertweet = EarlyStopping_trainer1.evaluate(test_ds_bertweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652a365-1f5b-44fd-aef6-c3e9920d712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname_bertweet, results_bertweet):\n",
    " print(f\"{setname_bertweet} Set Accuracy:\", round(results_bertweet[\"eval_accuracy\"], 3))\n",
    " print(f\"{setname_bertweet} Set Precision:\", round(results_bertweet[\"eval_precision\"], 3))\n",
    " print(f\"{setname_bertweet} Set Recall:\", round(results_bertweet[\"eval_recall\"], 3))\n",
    " print(f\"{setname_bertweet} Set F1 score:\", round(results_bertweet[\"eval_f1\"], 3))\n",
    "display_evaluation(\"Training\", train_results_bertweet)\n",
    "display_evaluation(\"Testing\", test_results_bertweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44960afc-e7ca-438a-aac6-addcb9f7a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, y, stratify=y, test_size=0.3, random_state=random_state)\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds_bertweet = Dataset.from_pandas(train_df)\n",
    "test_ds_bertweet = Dataset.from_pandas(test_df)\n",
    "train_ds_bertweet = train_ds_bertweet.map(tokenize_bertweet, batched=True)\n",
    "test_ds_bertweet = test_ds_bertweet.map(tokenize_bertweet, batched=True)\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Testing set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9f559-165b-4dc3-ac2a-66ed5f3a621c",
   "metadata": {},
   "source": [
    "## 4.2.2 Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e531d-1144-4f5b-8383-c2c3e88f5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name2  = 'roberta-base'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f40dc-48dc-4d74-8e07-8697c46411c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_roberta(batch):\n",
    "    return tokenizer2(batch['text'], truncation=True, padding=True)\n",
    "\n",
    "train_ds_roberta = train_ds.map(tokenize_roberta, batched=True)\n",
    "test_ds_roberta = test_ds.map(tokenize_roberta, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17a908-aca7-4f4c-922f-6c57b16b6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd5f75-31da-4342-8427-02c123eafd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a890985-cb38-40ad-b35a-2a04b818e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name2,\n",
    "    num_labels=df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990c6e2-a733-4d44-aa69-9fbfee6fc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopping_model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    " model_name2,\n",
    " num_labels=df[\"label\"].nunique(),\n",
    " problem_type=\"single_label_classification\")\n",
    "EarlyStopping_model2.train()\n",
    "EarlyStopping_training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=10,\n",
    " per_device_train_batch_size=16,\n",
    " per_device_eval_batch_size=64,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=2e-5,\n",
    " weight_decay=0.01,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " # Added for early stopping.\n",
    " metric_for_best_model = \"loss\",\n",
    " load_best_model_at_end = True\n",
    ")\n",
    "EarlyStopping_trainer2 = Trainer(\n",
    " model=EarlyStopping_model2,\n",
    " args=EarlyStopping_training_args,\n",
    " train_dataset=train_ds_roberta,\n",
    " eval_dataset=test_ds_roberta,\n",
    " processing_class=tokenizer2\n",
    "    ,\n",
    " data_collator=DataCollatorWithPadding(tokenizer2),\n",
    " compute_metrics=compute_metrics,\n",
    " callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "EarlyStopping_trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d9fe5-73c3-4f9b-af7d-0d727527a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.eval()\n",
    "# Evaluate the datasets.\n",
    "train_results_roberta = EarlyStopping_trainer2.evaluate(train_ds_roberta)\n",
    "test_results_roberta = EarlyStopping_trainer2.evaluate(test_ds_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd44bb4-556d-4e2d-b915-8949761cab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname_roberta, results_roberta):\n",
    " print(f\"{setname_roberta} Set Accuracy:\", round(results_roberta[\"eval_accuracy\"], 3))\n",
    " print(f\"{setname_roberta} Set Precision:\", round(results_roberta[\"eval_precision\"], 3))\n",
    " print(f\"{setname_roberta} Set Recall:\", round(results_roberta[\"eval_recall\"], 3))\n",
    " print(f\"{setname_roberta} Set F1 score:\", round(results_roberta[\"eval_f1\"], 3))\n",
    "display_evaluation(\"Training\", train_results_roberta)\n",
    "display_evaluation(\"Testing\", test_results_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf1967-a6e4-4eb5-8c39-758811c1858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, y, stratify=y, test_size=0.3, random_state=random_state)\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds_roberta = Dataset.from_pandas(train_df)\n",
    "test_ds_roberta = Dataset.from_pandas(test_df)\n",
    "train_ds_roberta = train_ds_roberta.map(tokenize_roberta, batched=True)\n",
    "test_ds_roberta = test_ds_roberta.map(tokenize_roberta, batched=True)\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Testing set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dd2fa-5513-4219-aa50-500a2c536d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns (matrix, tokens)\n",
    "def compute_attention_matrix(tokenizer, model, text):\n",
    "    # Feed into the model, you could also grab the token embedding directly\n",
    "    # from the dataset, in which case this step would be unnecessary. We want\n",
    "    # the output in Tensor format that we can feed to the model, so we use\n",
    "    # return_tensors=\"pt\" (PyTorch Tensor). Lastly, send the tensor to\n",
    "    # whichever device the model is located on. This is unnecessary if you\n",
    "    # are running purely on the CPU, but needed for models on GPUs.\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    # We use torch.no_grad() to ensure the weights in the model are unchanged.\n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens, output_attentions=True)\n",
    "    # Stack layers. Depending on your model, this may have no effect.\n",
    "    # Move it back to the GPU if it was previously on the GPU.\n",
    "    attentions = torch.stack(pred.attentions).cpu()\n",
    "    # Remove the batch dimension, as there is only a zero value there.\n",
    "    attentions = attentions.squeeze(1)\n",
    "    # Average over the transformer layers and heads.\n",
    "    attentions = attentions.mean(dim=0).mean(dim=0)\n",
    "    # attentions now contains a matrix of importance from every token to every\n",
    "    # other token. e.g. if the message contained 10 tokens, it would be 10x10.\n",
    "    # Select the predicted class.\n",
    "    pred_class = pred.logits.cpu().argmax(-1).item()\n",
    "    # Also return a string representation of the tokens in the message.\n",
    "    # Plotting the integer token IDs would not be very meaningful. \n",
    "    token_strs = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]) \n",
    "    return (attentions, pred_class, token_strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65774f37-5984-45d9-8095-db6af2c44bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attentions, tokens, title):\n",
    " # Enlarge figure to take up more of the width.\n",
    " plt.figure(figsize=(10, 8))\n",
    " plt.title(title)\n",
    " # Plot heatmap.\n",
    " sns.heatmap(\n",
    " attentions, # Plot our attention matrix.\n",
    " xticklabels=tokens, # Display token names on X axis.\n",
    " yticklabels=tokens, # Display token names on Y axis.\n",
    " cmap='binary', # Black for low, white for high\n",
    " cbar=True # Display colour bar.\n",
    " )\n",
    " \n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddee34-b302-4f41-866a-415e4e72a483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7bafe-42b3-4917-9172-e25541289a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention_matrix(tokenizer, model, text, model_name=\"Model\"):\n",
    " attention, pred_class, tokens = compute_attention_matrix(tokenizer, model, text)\n",
    " pred_label = \"Positive\" if pred_class == 1 else \"Negative\"\n",
    " title=f\"{model_name}\\n{text}\\nPredicted class: {pred_label}\"\n",
    " plot_attention(attention, tokens, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefcc3f-8862-40a4-b306-0fb481dc6057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9ed6d-82e6-4a6b-bd1c-c493424cd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizer1, model1, df[df[\"label\"] == 0].iloc[18][\"text\"], model_name=\"BERTweet Model\")\n",
    "display_attention_matrix(tokenizer2, model2, df[df[\"label\"] == 0].iloc[18][\"text\"], model_name=\"RoBERTa Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4759094-a252-49a6-9768-97c1080e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizer1, model1, df[df[\"label\"] == 0].iloc[72][\"text\"], model_name=\"BERTweet Model\")\n",
    "display_attention_matrix(tokenizer2, model2, df[df[\"label\"] == 0].iloc[72][\"text\"], model_name=\"RoBERTa Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3835c-f8cb-44bb-a64b-1d627ceee512",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bertweet = EarlyStopping_trainer1.predict(test_ds_bertweet)\n",
    "pred_roberta = EarlyStopping_trainer2.predict(test_ds_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2af22-7f95-46a2-90e7-2c1764bc843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor, apply softmax, and convert back to a numpy array.\n",
    "pred_probs_bertweet = torch.nn.functional.softmax(torch.Tensor(pred_bertweet.predictions)).numpy()\n",
    "pred_probs_roberta = torch.nn.functional.softmax(torch.Tensor(pred_roberta.predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4dc02f-42bf-4740-b74f-58eb661431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ROC index. Recall y_test contains our original labels for the testing set.\n",
    "roc_index_bertweet = roc_auc_score(y_test, pred_probs_bertweet[:, 1])\n",
    "roc_index_roberta = roc_auc_score(y_test, pred_probs_roberta[:, 1])\n",
    "# Compute the ROC curve.\n",
    "fpr_bertweet,tpr_bertweet, thresholds_bertweet = roc_curve(y_test, pred_probs_bertweet[:,1])\n",
    "fpr_roberta,tpr_roberta, thresholds_roberta = roc_curve(y_test, pred_probs_roberta[:,1])\n",
    "# And plot it on a line graph, similarly to what we did in previous weeks.\n",
    "plt.plot(fpr_bertweet, tpr_bertweet, label=\"BERTweet Model: {:.3f}\".format(roc_index_bertweet),\n",
    "color='red', lw=0.5)\n",
    "plt.plot(fpr_roberta, tpr_roberta, label=\"RoBerta Model: {:.3f}\".format(roc_index_roberta),\n",
    "color='navy', lw=0.5)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic for positive sentiment\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b56ea-869f-4444-b2a4-caecd50aadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf_df = pd.read_csv('tfidf_features_small.csv')\n",
    "tfidf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e55528-0e1b-4a84-a945-3467c4cbb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df\n",
    "\n",
    "df= pd.read_csv(\"hydrogen_small.csv\")\n",
    "y= df['label'].values\n",
    "\n",
    "random_state = 42\n",
    "test_set_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=test_set_size, stratify=y, \n",
    "                                                    random_state=random_state)\n",
    "model = LogisticRegression(random_state=random_state)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5db81-9cb6-4182-b1f9-11676ee1cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# classification report on test data\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3acc39-987d-4180-9804-a086c7daf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'Irrelevant': 0, 'Relevant': 1}\n",
    "y_test_bin = np.array([label_map[y] for y in y_test])\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC/AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_pred_proba) \n",
    "roc_auc = roc_auc_score(y_test_bin, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd8eb0-3b39-4b7a-9b7a-b2ccbcd0550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# BERTweet ROC\n",
    "plt.plot(fpr_bertweet, tpr_bertweet,\n",
    "         label=f'BERTweet Model (AUC = {roc_index_bertweet:.3f})',\n",
    "         color='red', lw=1.5)\n",
    "#Roberta ROV\n",
    "plt.plot(fpr_roberta, tpr_roberta,\n",
    "         label=f'Roberta Model (AUC = {roc_index_roberta:.3f})',\n",
    "         color='blue', lw=1.5)\n",
    "\n",
    "plt.plot(fpr, tpr,\n",
    "         label=f'Logistic Regression (AUC = {roc_auc:.3f})',\n",
    "         color='black', lw=1.5)\n",
    "\n",
    "plt.plot([0,1], [0,1], color='grey', linestyle='--', lw=1)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.title(\"ROC Curve Comparison: BERTweet vs Logistic Regression\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff9235-c440-42a5-82be-5c0d8085f976",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca4cd1-e5af-4b11-9f1b-2890731565bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7f573-96e4-4308-88a2-efdacc5e4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c070b-e25d-4095-b510-e961386856b8",
   "metadata": {},
   "source": [
    "## 5.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee5142-7370-4736-bd74-a9c22a1ae223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess SQuAD dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "print(\"Number of training examples:\", len(dataset['train']))\n",
    "print(\"Number of validation examples:\", len(dataset['validation']))\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91cd44-401f-40d0-843d-b9c3c1159353",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36ec45-b391-4e22-b008-c0e3d41c7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Filtering\n",
    "filtered_dataset = dataset.filter(lambda x: x[\"answers\"]['text'][0].upper() !=\"CANNOTANSWER\")\n",
    "print(\"Size of training set after removing unanswerable questions:\", len(filtered_dataset['train']))\n",
    "print(\"Size of validation set after removing unanswerable questions:\", len(filtered_dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245f178-e3bd-4529-b0b0-d709411c84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Testing Set\n",
    "\n",
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set\n",
    "\n",
    "training_set = train_dataset\n",
    "validation_set = val_dataset\n",
    "testing_set = test_dataset\n",
    "\n",
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8779f14-ad23-4e6c-999b-1f9405cbd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Tokenizer\n",
    "MODEL_NAME = \"t5-small\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0ce81-a755-4f81-a3e5-afdb344aab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question}  context: {context}\"\n",
    "\n",
    "# Obtains the context, question and answer from a given sample.\n",
    "def extract_sample_parts(sample):\n",
    "    context = sample[\"context\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answers\"]['text'][0]\n",
    "    question_with_context = encode_question_and_context(question, context)\n",
    "    return (question_with_context, question, answer)\n",
    "\n",
    "# Encodes the sample, returning token IDs.\n",
    "def preprocess(sample):\n",
    "    # Extract data from sample.\n",
    "    question_with_context, question, answer = extract_sample_parts(sample)\n",
    "\n",
    "    # Using truncation causes the tokenizer to emit a warning for every sample.\n",
    "    # This will generate a significant amount of messages, and likely crash\n",
    "    # your browser tab. We temporarily disable log messages to work around this.\n",
    "    # See https://github.com/huggingface/transformers/issues/14285\n",
    "    old_level = transformers_logging.get_verbosity()\n",
    "    transformers_logging.set_verbosity_error()\n",
    "    \n",
    "    # Generate tokens for the input.\n",
    "    # We include both the context and the question (first two parameters).\n",
    "    input_tokens = tokenizer(question_with_context, question, padding=\"max_length\",\n",
    "                             truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Generate tokens for the expected answer. There is no need to include the \n",
    "    output_tokens = tokenizer(answer, padding=\"max_length\", truncation=True,\n",
    "                              max_length=MAX_OUTPUT_LENGTH)\n",
    "\n",
    "    # Restore old logging level, see above.\n",
    "    transformers_logging.set_verbosity(old_level)\n",
    "\n",
    "    # The output of the tokenizer is a map containing {input_ids, attention_mask}.\n",
    "    # For trianing, we need to add the labels (answer/output tokens) to the map.\n",
    "    input_tokens[\"labels\"] = np.array(output_tokens[\"input_ids\"])\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab943f-0b1d-4557-9312-f0461e1ff062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "training_set_enc = train_dataset.map(preprocess, batched=False)\n",
    "validation_set_enc = val_dataset.map(preprocess, batched=False)\n",
    "testing_set_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64895a57-4872-4271-8df5-0f06d4527e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare 20 data points for qualitative analysis\n",
    "q_data = test_dataset.select(range(20))\n",
    "q_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475195a-441c-41be-addc-a69c8d90457d",
   "metadata": {},
   "source": [
    "## 5.2 Fine-tuning the T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08eb4a-a0d4-487f-8dda-3abf777071aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model\n",
    "# Ensure the resources for any existing model have been freed.\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5172091-95d2-4749-8937-1f98f19ffc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "training_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "validation_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "testing_set_enc.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbfbd6-ca28-417d-94fa-21456ad43f1f",
   "metadata": {},
   "source": [
    "### First Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457855e4-37e0-48d1-920f-0706163790e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# hyperparameter: setting1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3, #5-10\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4, \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model = \"loss\", \n",
    "    load_best_model_at_end = True #Early Stopping\n",
    ")\n",
    "\n",
    "# Train T5 model: setting1\n",
    "\n",
    "model.train()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_set_enc,\n",
    "    eval_dataset=validation_set_enc,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)], #Early Stopping\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee05404-5d5d-4711-9644-852630eb0a72",
   "metadata": {},
   "source": [
    "### Second Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f074b6-d055-4a4d-ad52-1849702f546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e6ccd-6cc9-40e7-b2a3-5fbc473734b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: setting2\n",
    "\n",
    "training_args_change = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=8, \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    metric_for_best_model = \"loss\", \n",
    "    load_best_model_at_end = True #Early Stopping\n",
    ")\n",
    "\n",
    "# Train T5 model: setting2\n",
    "\n",
    "model.train()\n",
    "trainer_change = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_change,\n",
    "    train_dataset=training_set_enc,\n",
    "    eval_dataset=validation_set_enc,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)], #Early Stopping\n",
    ")\n",
    "\n",
    "trainer_change.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b94e6e-b89a-4f96-84c6-a94ac5ca8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "trainer.save_model(\"t5_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c683e8-d39b-4829-b936-d4ddf3ad1425",
   "metadata": {},
   "source": [
    "## 5.3 Evaluation the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ead148-3bd0-43b0-8e6b-b8f2c3931d5b",
   "metadata": {},
   "source": [
    "#### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d8658-3c82-4faa-b301-9eccc992e2c8",
   "metadata": {},
   "source": [
    "- display_evaluation(setname, results)\n",
    "- generate_response(tokenizer, model, question)\n",
    "- generate_answers(tokenizer, model, dataset, use_context=True, limit=None)\n",
    "- display_answer_and_references(question, answer, reference)\n",
    "- compute_average_score(scores, metric, key)\n",
    "- compute_rouge(predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d6e91-bc36-440c-bda5-c1ba1548420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 3))\n",
    "\n",
    "# Generates a response for a single input/question.\n",
    "def generate_response(tokenizer, model, question):\n",
    "    # Convert the sentences into a list of numeric tokens. We instruct the tokenizer\n",
    "    # to return PyTorch tensors (\"pt\") so that we can feed them directly into the model.\n",
    "    tokenized = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                          max_length=MAX_OUTPUT_LENGTH).to(model.device)\n",
    "    # Generate outputs using the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "        \n",
    "    # The model outputs a list of numeric tokens. To convert these tokens back to\n",
    "    # sentences, we can use the batch_decode function from the tokenizer.\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "    \n",
    "# Generates a list of responses from the specified model, optionally including\n",
    "# the context in the prompt. If limit is set, then answers will only be generated\n",
    "# for the first N questions of the dataset.\n",
    "def generate_answers(tokenizer, model, dataset, use_context=True, limit=None):\n",
    "    # Subsampling if requested.\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "        \n",
    "    # Create list of encoded tokens, similarly to how we preprocessed the data for\n",
    "    # training. We do this so we can use batch processing to speed up inference.\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "    for sample in dataset:\n",
    "        question_with_context, question, answer = extract_sample_parts(sample)\n",
    "        \n",
    "        # Only include the context if the caller requested it.\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "            \n",
    "        # Include the original question/answer.\n",
    "        questions.append(question)\n",
    "        references.append(answer)\n",
    "        \n",
    "    # Generate responses for each of the prompts/inputs.\n",
    "    # Submitting each question to the model separately would significantly\n",
    "    # increase processing time, especially if the model is located on the GPU.\n",
    "    # Instead, we group questions together in the same batch size that we used\n",
    "    # for training.\n",
    "    outputs = []\n",
    "    for samples in batched(inputs, 128):\n",
    "        # Python's batched() function returns a tuple of the batch\n",
    "        # size, which we have to first convert to a list.\n",
    "        responses = generate_response(tokenizer, model, list(samples))\n",
    "        \n",
    "    # generate_responses() returns an equal-sized list of responses.\n",
    "    outputs.extend(responses)\n",
    "    \n",
    "    # The length of the reference responses should equal the length of the\n",
    "    # generated responses.\n",
    "    assert (len(outputs) == len(references))\n",
    "    return outputs, references, questions\n",
    "\n",
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57371fcd-633b-443c-a853-2440980e5073",
   "metadata": {},
   "source": [
    "#### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cf492-3bbe-429e-9169-2d1ddca59bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average score of a given metric from a list of ROUGE scores.\n",
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "        # Since it's not a map, we have to manually read the attribute.\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "    \n",
    "# Computes ROGUE-1, ROGUE-2 and ROGUE-L scores for the given generated\n",
    "# answers and reference answers.\n",
    "def compute_rouge(predictions, references):\n",
    "    # Compute ROUGE-1, ROGUE-2 and ROUGE-L.\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    \n",
    "    # Use Porter stemmer to strip word suffixes to improve matching.\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    "    \n",
    "    # For each answer/reference pair, compute the ROUGE metrics.\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "        \n",
    "    # Compute the average precision, recall and F1 score for each metric.\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(\n",
    "                scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e082a7-8133-4844-8a03-94a3c73679df",
   "metadata": {},
   "source": [
    "#### ROUGE Metrics: Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4dc6b-0a31-4575-8d76-10deafa22749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the datasets.\n",
    "display_evaluation(\"Training\", trainer.evaluate(training_set_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(testing_set_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e85d6-c0a9-4af3-894b-555d326b700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(tokenizer, model, testing_set, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(tokenizer, model, testing_set, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cef96-e74b-4e56-88be-db4292b7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874c83b-2220-49a5-9b91-180d988ece11",
   "metadata": {},
   "source": [
    "## 5.4 Generative Avalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47baf3d-38da-4d10-9dad-6dc4c9e5d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Generative Analysis\n",
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "\n",
    "# 5.4.a question + context\n",
    "print(\"*** With context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i],\n",
    "                                  refs_ctx[i])\n",
    "\n",
    "# 5.4.b question\n",
    "print(\"*** Without context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i],\n",
    "                                  answers_noctx[i], refs_noctx[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f66ce8-d320-4374-b585-cc41f4ef8dad",
   "metadata": {},
   "source": [
    "## 5.5 Comparison with a Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca278f40-4020-481c-a4c2-89ffe61a2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f9fe3-b146-4891-955f-6f281b1da17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set\n",
    "\n",
    "training_set = train_dataset\n",
    "validation_set = val_dataset\n",
    "testing_set = test_dataset\n",
    "\n",
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43343d5e-083b-4a4f-bd8a-0313582529a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mrm8488/t5-base-finetuned-squadv2\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44d4db-95ac-474a-aef3-a14d2166ba36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ebeac-d43a-4385-b4d7-f04b495dba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8750d-cc6d-4d39-84bb-b7bb9c54be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fda40-d97f-4b85-94bf-ed7c7c2358e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(\n",
    "    tokenizer, model, testing_set, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(\n",
    "    tokenizer, model, testing_set, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51632d99-6621-454d-98cb-27a7fcb1aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefb2da-81e5-4849-945c-de90dba31393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc87e1c-ff1b-4de1-a42f-b966d5059c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b589354-cf92-4d17-af0c-5777198cd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "      \n",
    "print(\"*** With context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i],\n",
    "                                  refs_ctx[i])\n",
    "\n",
    "# 5.4.b question\n",
    "print(\"*** Without context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i],\n",
    "                                  answers_noctx[i], refs_noctx[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
