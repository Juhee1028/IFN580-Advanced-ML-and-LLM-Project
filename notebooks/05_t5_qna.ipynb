{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ff9235-c440-42a5-82be-5c0d8085f976",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca4cd1-e5af-4b11-9f1b-2890731565bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7f573-96e4-4308-88a2-efdacc5e4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c070b-e25d-4095-b510-e961386856b8",
   "metadata": {},
   "source": [
    "## 5.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee5142-7370-4736-bd74-a9c22a1ae223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess SQuAD dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "print(\"Number of training examples:\", len(dataset['train']))\n",
    "print(\"Number of validation examples:\", len(dataset['validation']))\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91cd44-401f-40d0-843d-b9c3c1159353",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36ec45-b391-4e22-b008-c0e3d41c7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Filtering\n",
    "filtered_dataset = dataset.filter(lambda x: x[\"answers\"]['text'][0].upper() !=\"CANNOTANSWER\")\n",
    "print(\"Size of training set after removing unanswerable questions:\", len(filtered_dataset['train']))\n",
    "print(\"Size of validation set after removing unanswerable questions:\", len(filtered_dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245f178-e3bd-4529-b0b0-d709411c84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Testing Set\n",
    "\n",
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set\n",
    "\n",
    "training_set = train_dataset\n",
    "validation_set = val_dataset\n",
    "testing_set = test_dataset\n",
    "\n",
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8779f14-ad23-4e6c-999b-1f9405cbd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Tokenizer\n",
    "MODEL_NAME = \"t5-small\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0ce81-a755-4f81-a3e5-afdb344aab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question}  context: {context}\"\n",
    "\n",
    "# Obtains the context, question and answer from a given sample.\n",
    "def extract_sample_parts(sample):\n",
    "    context = sample[\"context\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answers\"]['text'][0]\n",
    "    question_with_context = encode_question_and_context(question, context)\n",
    "    return (question_with_context, question, answer)\n",
    "\n",
    "# Encodes the sample, returning token IDs.\n",
    "def preprocess(sample):\n",
    "    # Extract data from sample.\n",
    "    question_with_context, question, answer = extract_sample_parts(sample)\n",
    "\n",
    "    # Using truncation causes the tokenizer to emit a warning for every sample.\n",
    "    # This will generate a significant amount of messages, and likely crash\n",
    "    # your browser tab. We temporarily disable log messages to work around this.\n",
    "    # See https://github.com/huggingface/transformers/issues/14285\n",
    "    old_level = transformers_logging.get_verbosity()\n",
    "    transformers_logging.set_verbosity_error()\n",
    "    \n",
    "    # Generate tokens for the input.\n",
    "    # We include both the context and the question (first two parameters).\n",
    "    input_tokens = tokenizer(question_with_context, question, padding=\"max_length\",\n",
    "                             truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Generate tokens for the expected answer. There is no need to include the \n",
    "    output_tokens = tokenizer(answer, padding=\"max_length\", truncation=True,\n",
    "                              max_length=MAX_OUTPUT_LENGTH)\n",
    "\n",
    "    # Restore old logging level, see above.\n",
    "    transformers_logging.set_verbosity(old_level)\n",
    "\n",
    "    # The output of the tokenizer is a map containing {input_ids, attention_mask}.\n",
    "    # For trianing, we need to add the labels (answer/output tokens) to the map.\n",
    "    input_tokens[\"labels\"] = np.array(output_tokens[\"input_ids\"])\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab943f-0b1d-4557-9312-f0461e1ff062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "training_set_enc = train_dataset.map(preprocess, batched=False)\n",
    "validation_set_enc = val_dataset.map(preprocess, batched=False)\n",
    "testing_set_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64895a57-4872-4271-8df5-0f06d4527e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare 20 data points for qualitative analysis\n",
    "q_data = test_dataset.select(range(20))\n",
    "q_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475195a-441c-41be-addc-a69c8d90457d",
   "metadata": {},
   "source": [
    "## 5.2 Fine-tuning the T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08eb4a-a0d4-487f-8dda-3abf777071aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model\n",
    "# Ensure the resources for any existing model have been freed.\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5172091-95d2-4749-8937-1f98f19ffc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "training_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "validation_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "testing_set_enc.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbfbd6-ca28-417d-94fa-21456ad43f1f",
   "metadata": {},
   "source": [
    "### First Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457855e4-37e0-48d1-920f-0706163790e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# hyperparameter: setting1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3, #5-10\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4, \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model = \"loss\", \n",
    "    load_best_model_at_end = True #Early Stopping\n",
    ")\n",
    "\n",
    "# Train T5 model: setting1\n",
    "\n",
    "model.train()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_set_enc,\n",
    "    eval_dataset=validation_set_enc,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)], #Early Stopping\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee05404-5d5d-4711-9644-852630eb0a72",
   "metadata": {},
   "source": [
    "### Second Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f074b6-d055-4a4d-ad52-1849702f546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e6ccd-6cc9-40e7-b2a3-5fbc473734b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: setting2\n",
    "\n",
    "training_args_change = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=8, \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    metric_for_best_model = \"loss\", \n",
    "    load_best_model_at_end = True #Early Stopping\n",
    ")\n",
    "\n",
    "# Train T5 model: setting2\n",
    "\n",
    "model.train()\n",
    "trainer_change = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_change,\n",
    "    train_dataset=training_set_enc,\n",
    "    eval_dataset=validation_set_enc,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)], #Early Stopping\n",
    ")\n",
    "\n",
    "trainer_change.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b94e6e-b89a-4f96-84c6-a94ac5ca8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "trainer.save_model(\"t5_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c683e8-d39b-4829-b936-d4ddf3ad1425",
   "metadata": {},
   "source": [
    "## 5.3 Evaluation the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ead148-3bd0-43b0-8e6b-b8f2c3931d5b",
   "metadata": {},
   "source": [
    "#### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d8658-3c82-4faa-b301-9eccc992e2c8",
   "metadata": {},
   "source": [
    "- display_evaluation(setname, results)\n",
    "- generate_response(tokenizer, model, question)\n",
    "- generate_answers(tokenizer, model, dataset, use_context=True, limit=None)\n",
    "- display_answer_and_references(question, answer, reference)\n",
    "- compute_average_score(scores, metric, key)\n",
    "- compute_rouge(predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d6e91-bc36-440c-bda5-c1ba1548420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 3))\n",
    "\n",
    "# Generates a response for a single input/question.\n",
    "def generate_response(tokenizer, model, question):\n",
    "    # Convert the sentences into a list of numeric tokens. We instruct the tokenizer\n",
    "    # to return PyTorch tensors (\"pt\") so that we can feed them directly into the model.\n",
    "    tokenized = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                          max_length=MAX_OUTPUT_LENGTH).to(model.device)\n",
    "    # Generate outputs using the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "        \n",
    "    # The model outputs a list of numeric tokens. To convert these tokens back to\n",
    "    # sentences, we can use the batch_decode function from the tokenizer.\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "    \n",
    "# Generates a list of responses from the specified model, optionally including\n",
    "# the context in the prompt. If limit is set, then answers will only be generated\n",
    "# for the first N questions of the dataset.\n",
    "def generate_answers(tokenizer, model, dataset, use_context=True, limit=None):\n",
    "    # Subsampling if requested.\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "        \n",
    "    # Create list of encoded tokens, similarly to how we preprocessed the data for\n",
    "    # training. We do this so we can use batch processing to speed up inference.\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "    for sample in dataset:\n",
    "        question_with_context, question, answer = extract_sample_parts(sample)\n",
    "        \n",
    "        # Only include the context if the caller requested it.\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "            \n",
    "        # Include the original question/answer.\n",
    "        questions.append(question)\n",
    "        references.append(answer)\n",
    "        \n",
    "    # Generate responses for each of the prompts/inputs.\n",
    "    # Submitting each question to the model separately would significantly\n",
    "    # increase processing time, especially if the model is located on the GPU.\n",
    "    # Instead, we group questions together in the same batch size that we used\n",
    "    # for training.\n",
    "    outputs = []\n",
    "    for samples in batched(inputs, 128):\n",
    "        # Python's batched() function returns a tuple of the batch\n",
    "        # size, which we have to first convert to a list.\n",
    "        responses = generate_response(tokenizer, model, list(samples))\n",
    "        \n",
    "    # generate_responses() returns an equal-sized list of responses.\n",
    "    outputs.extend(responses)\n",
    "    \n",
    "    # The length of the reference responses should equal the length of the\n",
    "    # generated responses.\n",
    "    assert (len(outputs) == len(references))\n",
    "    return outputs, references, questions\n",
    "\n",
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57371fcd-633b-443c-a853-2440980e5073",
   "metadata": {},
   "source": [
    "#### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cf492-3bbe-429e-9169-2d1ddca59bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average score of a given metric from a list of ROUGE scores.\n",
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "        # Since it's not a map, we have to manually read the attribute.\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "    \n",
    "# Computes ROGUE-1, ROGUE-2 and ROGUE-L scores for the given generated\n",
    "# answers and reference answers.\n",
    "def compute_rouge(predictions, references):\n",
    "    # Compute ROUGE-1, ROGUE-2 and ROUGE-L.\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    \n",
    "    # Use Porter stemmer to strip word suffixes to improve matching.\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    "    \n",
    "    # For each answer/reference pair, compute the ROUGE metrics.\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "        \n",
    "    # Compute the average precision, recall and F1 score for each metric.\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(\n",
    "                scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e082a7-8133-4844-8a03-94a3c73679df",
   "metadata": {},
   "source": [
    "#### ROUGE Metrics: Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4dc6b-0a31-4575-8d76-10deafa22749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the datasets.\n",
    "display_evaluation(\"Training\", trainer.evaluate(training_set_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(testing_set_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e85d6-c0a9-4af3-894b-555d326b700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(tokenizer, model, testing_set, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(tokenizer, model, testing_set, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cef96-e74b-4e56-88be-db4292b7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874c83b-2220-49a5-9b91-180d988ece11",
   "metadata": {},
   "source": [
    "## 5.4 Generative Avalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47baf3d-38da-4d10-9dad-6dc4c9e5d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Generative Analysis\n",
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "\n",
    "# 5.4.a question + context\n",
    "print(\"*** With context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i],\n",
    "                                  refs_ctx[i])\n",
    "\n",
    "# 5.4.b question\n",
    "print(\"*** Without context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i],\n",
    "                                  answers_noctx[i], refs_noctx[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f66ce8-d320-4374-b585-cc41f4ef8dad",
   "metadata": {},
   "source": [
    "## 5.5 Comparison with a Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca278f40-4020-481c-a4c2-89ffe61a2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f9fe3-b146-4891-955f-6f281b1da17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set\n",
    "\n",
    "training_set = train_dataset\n",
    "validation_set = val_dataset\n",
    "testing_set = test_dataset\n",
    "\n",
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43343d5e-083b-4a4f-bd8a-0313582529a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mrm8488/t5-base-finetuned-squadv2\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44d4db-95ac-474a-aef3-a14d2166ba36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ebeac-d43a-4385-b4d7-f04b495dba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8750d-cc6d-4d39-84bb-b7bb9c54be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fda40-d97f-4b85-94bf-ed7c7c2358e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(\n",
    "    tokenizer, model, testing_set, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(\n",
    "    tokenizer, model, testing_set, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51632d99-6621-454d-98cb-27a7fcb1aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefb2da-81e5-4849-945c-de90dba31393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc87e1c-ff1b-4de1-a42f-b966d5059c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b589354-cf92-4d17-af0c-5777198cd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "      \n",
    "print(\"*** With context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i],\n",
    "                                  refs_ctx[i])\n",
    "\n",
    "# 5.4.b question\n",
    "print(\"*** Without context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i],\n",
    "                                  answers_noctx[i], refs_noctx[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
